{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Automatic Wikipedia Extraction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, time\n",
    "import os, os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm_notebook\n",
    "except:\n",
    "    !conda install --yes --prefix {sys.prefix} tqdm\n",
    "    from tqdm import tqdm_notebook\n",
    "\n",
    "try:\n",
    "    import wikipedia\n",
    "except:\n",
    "    !conda install --yes --prefix {sys.prefix} wikipedia\n",
    "    import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the language of the articles you want\n",
    "and the target number of articles you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_language = 'french'\n",
    "target = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_and_abbr = {'czech': 'cz', 'danish': 'da', 'dutch': 'nl',\n",
    "                      'english': 'en', 'estonian': 'et', 'finnish': 'fi',\n",
    "                      'french': 'fr', 'german': 'de', 'greek': 'el',\n",
    "                      'italian': 'it', 'norwegian': 'no', 'polish': 'pl', \n",
    "                      'portuguese': 'pt', 'slovene': 'sl',\n",
    "                      'spanish': 'es', 'swedish': 'sv', 'turkish': 'tr',\n",
    "                 'japanese': 'ja', 'persian': 'fa', 'farsi': 'fa', 'tibetan': 'bo'}\n",
    "\n",
    "wikipedia.set_lang(langs_and_abbr[tokenization_language])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file output_dir already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Account for how many text files of the same language have already been extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f50ad21896445ebfb76c0528652eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=301), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 articles already exported, with 100 remaining\n"
     ]
    }
   ],
   "source": [
    "existing_files = next(os.walk('.\\\\output_dir'))[2]\n",
    "\n",
    "already_exported = 0\n",
    "\n",
    "for existing_file in tqdm_notebook(existing_files):\n",
    "    if existing_file[0:2] == langs_and_abbr[tokenization_language]:\n",
    "        already_exported += 1\n",
    "\n",
    "remaining = target - already_exported\n",
    "\n",
    "n_articles = already_exported\n",
    "\n",
    "print(n_articles, 'articles already exported, with', remaining, 'remaining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get random article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list_of_articles = []\n",
    "\n",
    "while remaining != 0:\n",
    "\n",
    "    if remaining >= 10:\n",
    "        ten_random_articles = wikipedia.random(pages=10)\n",
    "        for article in ten_random_articles:\n",
    "            if article not in final_list_of_articles:\n",
    "                final_list_of_articles.append(article)\n",
    "                remaining = remaining - 1\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    elif remaining < 10:\n",
    "        for n in range(remaining):\n",
    "            one_random_article = wikipedia.random(pages=1)\n",
    "            if one_random_article not in final_list_of_articles:\n",
    "                final_list_of_articles.append(one_random_article)\n",
    "                remaining = remaining - 1\n",
    "            else:\n",
    "                pass\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process and extract articles to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-10 16:59:22.520379 Wikipedia extraction process started for 100 files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982443460149447c95b392d128b0f0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file C:\\ProgramData\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-10 17:01:12.566839 error when extracting article Cooper Township\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now(), 'Wikipedia extraction process started for', target, 'files')\n",
    "for article_title in tqdm_notebook(final_list_of_articles):\n",
    "    \n",
    "    try:\n",
    "        article = wikipedia.page(article_title)\n",
    "    except:\n",
    "        final_list_of_articles.remove(article_title)\n",
    "        print(datetime.now(), 'error when extracting article', article_title)\n",
    "        \n",
    "        new_article = wikipedia.random(pages=1)\n",
    "        \n",
    "        while new_article in final_list_of_articles == True:\n",
    "            new_article = wikipedia.random(pages=1)\n",
    "        \n",
    "        final_list_of_articles.append(new_article)\n",
    "        pass\n",
    "    \n",
    "    n_articles += 1\n",
    "\n",
    "    output_filename = langs_and_abbr[tokenization_language] + '{:06}'.format(n_articles) + '.txt'\n",
    "\n",
    "    with open(os.path.join('.\\\\output_dir', output_filename), 'w', encoding='utf-8') as t:\n",
    "        t.write(article.content)\n",
    "\n",
    "print(datetime.now(), 'Wikipedia extraction process finished for', target, 'files')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

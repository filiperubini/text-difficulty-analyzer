{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Automatic Wikipedia Extraction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, time\n",
    "import os, os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm_notebook\n",
    "except:\n",
    "    try:\n",
    "        !conda install --yes --prefix {sys.prefix} tqdm\n",
    "    except:\n",
    "        !{sys.executable} -m pip install tqdm\n",
    "\n",
    "try:\n",
    "    import wikipedia\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the language of the articles you want\n",
    "and the target number of articles you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_language = 'persian'\n",
    "target = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_and_abbr = {'czech': 'cz', 'danish': 'da', 'dutch': 'nl',\n",
    "                      'english': 'en', 'estonian': 'et', 'finnish': 'fi',\n",
    "                      'french': 'fr', 'german': 'de', 'greek': 'el',\n",
    "                      'italian': 'it', 'norwegian': 'no', 'polish': 'pl', \n",
    "                      'portuguese': 'pt', 'slovene': 'sl', 'hebrew': 'he',\n",
    "                      'spanish': 'es', 'swedish': 'sv', 'turkish': 'tr',\n",
    "                 'japanese': 'ja', 'persian': 'fa', 'farsi': 'fa', 'tibetan': 'bo'}\n",
    "\n",
    "wikipedia.set_lang(langs_and_abbr[tokenization_language])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘output_dir’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Account for how many text files of the same language have already been extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2bc39f153284e08a30cc7945822f3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 articles already exported, with 90 remaining\n"
     ]
    }
   ],
   "source": [
    "existing_files = next(os.walk('./output_dir'))[2]\n",
    "\n",
    "already_exported = 0\n",
    "\n",
    "for existing_file in tqdm_notebook(existing_files):\n",
    "    if existing_file[0:2] == langs_and_abbr[tokenization_language]:\n",
    "        already_exported += 1\n",
    "\n",
    "remaining = target - already_exported\n",
    "\n",
    "n_articles = already_exported\n",
    "\n",
    "print(n_articles, 'articles already exported, with', remaining, 'remaining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get random article titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list_of_articles = []\n",
    "\n",
    "while remaining != 0:\n",
    "\n",
    "    if remaining >= 10:\n",
    "        ten_random_articles = wikipedia.random(pages=10)\n",
    "        for article in ten_random_articles:\n",
    "            if article not in final_list_of_articles:\n",
    "                final_list_of_articles.append(article)\n",
    "                remaining = remaining - 1\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    elif remaining < 10:\n",
    "        for n in range(remaining):\n",
    "            one_random_article = wikipedia.random(pages=1)\n",
    "            if one_random_article not in final_list_of_articles:\n",
    "                final_list_of_articles.append(one_random_article)\n",
    "                remaining = remaining - 1\n",
    "            else:\n",
    "                pass\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process and extract articles to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-11 09:08:21.083526 Wikipedia extraction process started for 100 files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49544fd01264f35bd55e23d9932615b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=90), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filipe/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/filipe/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-11 09:09:30.634846 error when extracting article MCB\n",
      "2018-09-11 09:10:25.375882 error when extracting article 白岩中学校\n",
      "2018-09-11 09:10:51.802080 error when extracting article キム・ハンソル\n",
      "2018-09-11 09:12:02.148702 error when extracting article ダンパ\n",
      "2018-09-11 09:15:27.221809 error when extracting article 李小鵬\n",
      "2018-09-11 09:15:39.284945 error when extracting article 大同区\n",
      "2018-09-11 09:16:41.757624 error when extracting article ソーン\n",
      "2018-09-11 09:20:32.323974 error when extracting article ウェーバー\n",
      "2018-09-11 09:21:13.958672 error when extracting article 遊場\n",
      "2018-09-11 09:21:39.882602 error when extracting article ヴィトー\n",
      "2018-09-11 09:22:56.213724 error when extracting article マスコミ\n",
      "2018-09-11 09:24:08.437702 error when extracting article キャメロン・カー\n",
      "\n",
      "2018-09-11 09:26:04.881371 Wikipedia extraction process finished for 100 files\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now(), 'Wikipedia extraction process started for', target, 'files')\n",
    "for article_title in tqdm_notebook(final_list_of_articles):\n",
    "    \n",
    "    try:\n",
    "        article = wikipedia.page(article_title)\n",
    "    except:\n",
    "        final_list_of_articles.remove(article_title)\n",
    "        print(datetime.now(), 'error when extracting article', article_title)\n",
    "        \n",
    "        new_article = wikipedia.random(pages=1)\n",
    "        \n",
    "        while new_article in final_list_of_articles == True:\n",
    "            new_article = wikipedia.random(pages=1)\n",
    "        \n",
    "        final_list_of_articles.append(new_article)\n",
    "        pass\n",
    "    \n",
    "    n_articles += 1\n",
    "\n",
    "    output_filename = langs_and_abbr[tokenization_language] + '{:06}'.format(n_articles) + '.txt'\n",
    "\n",
    "    with open(os.path.join('./output_dir', output_filename), 'w', encoding='utf-8') as t:\n",
    "        t.write(article.content)\n",
    "\n",
    "print(datetime.now(), 'Wikipedia extraction process finished for', target, 'files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, TreebankWordTokenizer, ngrams, WhitespaceTokenizer\n",
    "from itertools import accumulate, tee, chain\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from datetime import datetime, date, time\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from collections import defaultdict, OrderedDict\n",
    "import os, os.path\n",
    "import re\n",
    "import string\n",
    "from decimal import Decimal\n",
    "from decimal import *\n",
    "getcontext().prec = 6\n",
    "import statistics\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_language = 'portuguese'  # check the NLTK sentence tokenizers for available languages\n",
    "non_latin_alphabet = False # if your language uses a non-Latin alphabet, change to True\n",
    "files_dir_or_wiki = 'wiki' # change to 'list' if you have a list of filenames\n",
    "                           # or to 'dir' if you have a specific directory\n",
    "token_database = 'portuguese-tokens'\n",
    "exclude_numbers = True # True / False\n",
    "exclude_numbers_for_ngrams = False # True / False\n",
    "number_of_ngrams = 1 # choose up to x number of n-grams to extract (minimum = 1)\n",
    "threads = 4 # decrease this number if you have an old/low core processor (minimum = 1)\n",
    "chosen_encoding = 'utf-8-sig' # better utf-8-sig than utf-8, saves trouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if number_of_ngrams <= 1:\n",
    "    ngramrange = [1]\n",
    "elif number_of_ngrams > 1:\n",
    "    ngramrange = range(1,number_of_ngrams+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenization_language == 'japanese':\n",
    "    import tinysegmenter\n",
    "    tokenizer = tinysegmenter.TinySegmenter()\n",
    "elif non_latin_alphabet == True:\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "else:\n",
    "    tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token statistics\n",
    "client = MongoClient()\n",
    "db = client[token_database]\n",
    "\n",
    "token_stats = db[token_database]\n",
    "token_stats.allowDiskUse=True\n",
    "\n",
    "text_stats = token_stats.find_one({'_text-stats': True})['text_stats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number_repl_isdigit(s):\n",
    "    '''Returns True is string is a number,\n",
    "    i.e. if it contains dot or comma.'''\n",
    "    \n",
    "    return re.sub('[.,]', '', s).isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if non_latin_alphabet == False:\n",
    "\n",
    "    def cleantokensfromsentence(tokens, bar_numbers):\n",
    "        '''Clean a list of tokens to remove\n",
    "        extraneous characters and numerals.\n",
    "        Bar_numbers: if True, numbers will be deleted'''\n",
    "\n",
    "        clean_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "\n",
    "            if is_number_repl_isdigit(token) == True:\n",
    "                if bar_numbers == True:\n",
    "                    '''Barring numbers'''\n",
    "                    pass\n",
    "                elif bar_numbers == False:\n",
    "                    clean_tokens.append(token)\n",
    "            elif token.isalnum() is False:\n",
    "                if len(token) == 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    matches = 0\n",
    "                    for character in token:\n",
    "                        if character.isalnum() is False:\n",
    "                            matches += 1\n",
    "                    if matches == len(token):\n",
    "                        pass\n",
    "                    else:\n",
    "                        clean_tokens.append(token)\n",
    "            else:\n",
    "                clean_tokens.append(token)\n",
    "\n",
    "        return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if non_latin_alphabet == True:\n",
    "    \n",
    "    def cleantokensfromsentence(tokens, bar_numbers):\n",
    "        '''Clean a list of tokens to remove numerals.s'''\n",
    "\n",
    "        clean_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "\n",
    "            if is_number_repl_isdigit(token) == True:\n",
    "                if bar_numbers == True:\n",
    "                    '''Barring numbers'''\n",
    "                    pass\n",
    "                elif bar_numbers == False:\n",
    "                    clean_tokens.append(token)\n",
    "            else:\n",
    "                clean_tokens.append(token)\n",
    "\n",
    "        return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmultitokens(sentence, n_of_ngrams):\n",
    "    '''From a list of tokens, generate up to n n-grams'''\n",
    "    \n",
    "    all_ngrams = []\n",
    "    \n",
    "    for x in range(2, n_of_ngrams+1):\n",
    "        all_ngrams.extend(ngrams(sentence, x))\n",
    "        \n",
    "    return [' '.join(i) for i in list(chain(all_ngrams))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opentextfile(text_filename):\n",
    "    global chosen_encoding\n",
    "    with open(text_filename, 'r', encoding=chosen_encoding) as t:\n",
    "        return t.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dp(token):\n",
    "    \n",
    "    global text_stats, token_stats\n",
    "    \n",
    "    try:\n",
    "        # See if DP has already been calculated\n",
    "        dp = token_stats.find_one({'token': token})['dp']\n",
    "        return dp\n",
    "    \n",
    "    except TypeError:\n",
    "        # Token not found in DB\n",
    "        return float(1)\n",
    "    \n",
    "    except KeyError:\n",
    "        # DP was not calculated yet\n",
    "        # Let's calculate it then\n",
    "\n",
    "        try:\n",
    "            document = token_stats.find_one({'token': token})\n",
    "            token_length = document['len']\n",
    "            token_freq = document['freq']\n",
    "\n",
    "            freq_in_files = dict(zip(document['occurred_in'],document['freq_occurred_in']))\n",
    "\n",
    "            for file_id in freq_in_files:\n",
    "                freq_in_files[file_id] = Decimal(freq_in_files[file_id]) / Decimal(token_freq)\n",
    "\n",
    "            differences = float(0)\n",
    "\n",
    "            for file_id in text_stats['total'][str(token_length)+'-grams']:\n",
    "\n",
    "                expected_percentage = text_stats['total'][str(token_length)+'-grams'][file_id]['relfreq']\n",
    "                expected_percentage = abs(float(expected_percentage))\n",
    "                observed_percentage = abs(float(freq_in_files.get(file_id, float(0))))\n",
    "\n",
    "                diff = abs(expected_percentage - observed_percentage)\n",
    "                differences += diff\n",
    "            \n",
    "            dp = Decimal(differences) / Decimal(2)\n",
    "            \n",
    "            # Before returning DP, let us insert it to the DB\n",
    "            \n",
    "            result = token_stats.update_one({'token': token},\n",
    "                                    {'$set': {'dp': float(dp)}})\n",
    "\n",
    "            return float(dp)\n",
    "\n",
    "        except TypeError:\n",
    "            # Token not found in DB\n",
    "            return float(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Text Difficulty Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_tokens(text_as_string):\n",
    "    \n",
    "    global ngramrange\n",
    "    \n",
    "    gathered_tokens = {}\n",
    "    \n",
    "    for i in ngramrange:\n",
    "        gathered_tokens[str(i)+'-grams'] = []\n",
    "\n",
    "    # Convert to lowercase and into sentences\n",
    "\n",
    "    textlc = text_as_string.lower()\n",
    "    \n",
    "    sentences = sent_tokenize(textlc, language=tokenization_language)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        original_tokens = tokenizer.tokenize(sentence)\n",
    "        \n",
    "        clean_tokens = cleantokensfromsentence(original_tokens, exclude_numbers)\n",
    "        gathered_tokens['1-grams'].extend(clean_tokens)\n",
    "        \n",
    "        if ngramrange != [1]:\n",
    "                           \n",
    "            mtes = getmultitokens(cleantokensfromsentence(original_tokens, exclude_numbers_for_ngrams), number_of_ngrams)\n",
    "\n",
    "            for mte in mtes:\n",
    "                gathered_tokens[str(len(mte.split(' ')))+'-grams'].append(mte)\n",
    "    \n",
    "    # Turn tokens into Counter dicts\n",
    "    \n",
    "    for key in gathered_tokens:\n",
    "        gathered_tokens[key] = dict(Counter(gathered_tokens[key]))\n",
    "    \n",
    "    return gathered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dp(counter):\n",
    "    \n",
    "    token = list(counter.keys())[0]\n",
    "    \n",
    "    dp = calc_dp(token)\n",
    "    textfreq = list(counter.values())[0]\n",
    "    \n",
    "    for x in counter:\n",
    "        \n",
    "        return {'token': token,\n",
    "            'dp': dp,\n",
    "               'textfreq': textfreq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dp_values(gathered_tokens):\n",
    "    \n",
    "    global threads\n",
    "    \n",
    "    for ngram_len in gathered_tokens:\n",
    "        \n",
    "        counter_dict = gathered_tokens[ngram_len]\n",
    "        \n",
    "        counter_list = [{x:counter_dict[x]} for x in counter_dict]\n",
    "        \n",
    "        pool = ThreadPool(threads)\n",
    "        results = tqdm_notebook(pool.map(get_dp, counter_list))\n",
    "        pool.close()\n",
    "        \n",
    "        for result in results:\n",
    "            gathered_tokens[ngram_len][result['token']] = {'textfreq': result['textfreq'],\n",
    "                                            'dp': result['dp']}\n",
    "    \n",
    "    return gathered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tds(text_as_string):\n",
    "    \n",
    "    retrieved_tokens = retrieve_dp_values(gather_tokens(text_as_string))\n",
    "\n",
    "    for ngram_len in retrieved_tokens:\n",
    "\n",
    "        token_data = retrieved_tokens[ngram_len]\n",
    "\n",
    "        dp_values = []\n",
    "        unique_dp_values = []\n",
    "\n",
    "        for token in token_data:\n",
    "\n",
    "            textfreq = token_data[token]['textfreq']\n",
    "\n",
    "            try:\n",
    "                dp_value = float(token_data[token]['dp'])\n",
    "                \n",
    "                if dp_value != None:\n",
    "                    unique_dp_values.append(dp_value)\n",
    "                \n",
    "                for f in range(textfreq):\n",
    "                    if dp_value != None:\n",
    "                        # Disregard tokens that weren't found in the DB\n",
    "                        dp_values.append(dp_value)\n",
    "            except TypeError:\n",
    "                # Token does not exist on DB\n",
    "                pass\n",
    "\n",
    "        if len(dp_values) != 0 and len(dp_values) >= 2:\n",
    "            retrieved_tokens[ngram_len]['_stats_'] = {'total_dp_values':\n",
    "                                                      \n",
    "                                                      {'median': statistics.median(dp_values),\n",
    "                                                   'mean': statistics.mean(dp_values),\n",
    "                                                      },\n",
    "                                                      \n",
    "                                                     'unique_dp_values': \n",
    "                                                     \n",
    "                                                     {'median': statistics.median(unique_dp_values),\n",
    "                                                   'mean': statistics.mean(unique_dp_values),\n",
    "                                                     }}\n",
    "\n",
    "        elif len(dp_values) == 0 or len(dp_values) < 2:\n",
    "            print(datetime.now(), 'statistics not computed')\n",
    "            \n",
    "    # The calculation of the TDS value below only considers 1-grams\n",
    "    # since they're much more important than longer n-grams\n",
    "    # but you can adapt it to include 2-grams and 3-grams\n",
    "            \n",
    "    median_total = retrieved_tokens['1-grams']['_stats_']['total_dp_values']['median']\n",
    "    median_unique = retrieved_tokens['1-grams']['_stats_']['unique_dp_values']['median']\n",
    "    \n",
    "    retrieved_tokens['tds'] = statistics.mean([median_total, median_unique])\n",
    "\n",
    "    return retrieved_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficulty Highlighter (HTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export as HTML a difficulty-highlighted version of the text.<br>\n",
    "It exports the HTML to the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difficulty_highlighter(text_filename):\n",
    "    \n",
    "    parameter = 'markdown_colors'\n",
    "    \n",
    "    text_as_string = opentextfile(text_filename)\n",
    "    \n",
    "    text_as_string = re.sub('[“”]', '\"', text_as_string)\n",
    "    text_as_string = re.sub('[‘’]', \"'\", text_as_string)\n",
    "    \n",
    "    print(datetime.now(), 'text file opened')\n",
    "    \n",
    "    retrieved_tokens = calc_tds(text_as_string)\n",
    "    \n",
    "    print(datetime.now(), 'tokens retrieved')\n",
    "    \n",
    "    tds = retrieved_tokens.pop('tds', None)\n",
    "    stats = retrieved_tokens['1-grams'].pop('_stats_', None)\n",
    "    \n",
    "    difficulty_ranges = OrderedDict({'very easy': [0, 0.7],\n",
    "                                     'easy': [0.7, 0.80],\n",
    "                                     'average': [0.80, 0.90],\n",
    "                                     'slightly difficult': [0.90, 0.95],\n",
    "                                     'difficult': [0.95, 0.99],\n",
    "                                     'very difficult': [0.99, 1.00]})\n",
    "\n",
    "    parameters = OrderedDict({'html_colors': {'very easy': 'color:LightGrey;',\n",
    "                             'easy': 'color:green;',\n",
    "                             'average': 'color:blue;',\n",
    "                             'difficult': 'color:black;',\n",
    "                                              'slightly difficult': 'color:black;',\n",
    "                             'very difficult': 'color:Crimson;'},\n",
    "                  \n",
    "                  'markdown': {'very easy': 'font-style: normal;',\n",
    "                             'easy': 'font-style: normal;',\n",
    "                             'average': 'text-decoration: italic;',\n",
    "                               'slightly difficult': 'color:black;',\n",
    "                             'difficult': 'font-weight: bold;',\n",
    "                             'very difficult': 'font-variant: small-caps;letter-spacing: 1.5px'},\n",
    "                 \n",
    "                 'markdown_colors': {'very easy': 'font-style: normal;',\n",
    "                                     'easy': 'font-style: italic;',\n",
    "                                     'average': 'font-weight: 600;',\n",
    "                                     'slightly difficult': 'font-weight: bold;color:navy;',\n",
    "                                     'difficult': 'font-variant: small-caps;letter-spacing: 1.5px',\n",
    "                                     'very difficult': 'font-variant: small-caps;font-weight:bold;letter-spacing: 1.5px;color:Crimson'}})\n",
    "    \n",
    "    \n",
    "    for ngram_len in retrieved_tokens:\n",
    "        \n",
    "        for token in retrieved_tokens[ngram_len]:\n",
    "            \n",
    "            retrieved_tokens[ngram_len][token] = retrieved_tokens[ngram_len][token]['dp']\n",
    "    \n",
    "    token_ranges = {x:[] for x in difficulty_ranges}\n",
    "    \n",
    "    difficulties = []\n",
    "    tokens = []\n",
    "    dp_values = []\n",
    "    \n",
    "    for token in retrieved_tokens['1-grams']:\n",
    "        \n",
    "        token_dp = retrieved_tokens['1-grams'][token]\n",
    "        \n",
    "        for difficulty in difficulty_ranges:\n",
    "            \n",
    "            diffrange = difficulty_ranges[difficulty]\n",
    "            \n",
    "            if token_dp > diffrange[0] and token_dp <= diffrange[1]:\n",
    "                \n",
    "                difficulties.append(difficulty)\n",
    "                token_modified = \"(\" + r\"\\b\" + token + r\"\\b\" + \")\"\n",
    "                tokens.append(token_modified)\n",
    "                dp_values.append(token_dp)\n",
    "                token_ranges[difficulty].append(token)\n",
    "    \n",
    "    tokens_found = '<hr><h3>Wordlist:</h3><ol>'\n",
    "    for token_range in token_ranges:\n",
    "        tokens_found+= \"\".join([\"<li>\",\n",
    "                                \"<font style='font-size:small;%s'>\" % parameters[parameter][token_range],\n",
    "                               token_range.capitalize(),\n",
    "                                \":</font>\",\n",
    "                                \" <font style='font-size:small;font-style:italic;'>\",\n",
    "                                ' | '.join(sorted(token_ranges[token_range])),\n",
    "                                '</font></li>',\n",
    "                               ])\n",
    "    tokens_found += '</ol>'\n",
    "    \n",
    "    dp_values = [str(x)[:5] for x in dp_values]\n",
    "    \n",
    "    colors = [parameters[parameter][x] for x in difficulties]\n",
    "    \n",
    "    compiler = '|'.join(tokens)\n",
    "    regex = re.compile(compiler, re.I)\n",
    "\n",
    "    i = 0\n",
    "    output = \"\"\"<!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "    <link href=\"https://fonts.googleapis.com/css?family=Vollkorn:400,400i,600,600i,700,700i,900,900i&amp;subset=latin-ext\" rel=\"stylesheet\">\n",
    "    <style>\n",
    "    body {\n",
    "    font-family: 'Vollkorn', 'Georgia', serif;\n",
    "    }\n",
    "    </style>\n",
    "\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Text Difficulty Analyzer v. 1.00</h1>\n",
    "    <h3><i>Difficulty Highlighter</i></h3>\n",
    "    Filename: \"\"\"\n",
    "    \n",
    "    output = re.sub('\\n', '', output)\n",
    "    \n",
    "    output += text_filename + '<br>Text Difficulty Value: <b>'+ str(tds)[:8] + '</b><hr>'\n",
    "\n",
    "    legend = '<b>LEGEND:</b> '\n",
    "    \n",
    "    for x in parameters[parameter]:\n",
    "        \n",
    "        legend += \"\".join([\"<font style='\",\n",
    "                           parameters[parameter][x],\n",
    "                           \"'>\",\n",
    "                           x.capitalize(),\n",
    "                          '</font> | '])\n",
    "    \n",
    "    output += legend + \"<hr><h3><font style='font-variant:small-caps;letter-spacing: 1.5px'>Highlighted Text:</font></h3>\"\n",
    "    \n",
    "    print(datetime.now(), 'initiating difficulty highlighter')\n",
    "\n",
    "    for m in regex.finditer(text_as_string):\n",
    "\n",
    "        output += \"\".join([text_as_string[i:m.start()],\n",
    "                               \"<font style='%s'>\" % colors[m.lastindex-1],\n",
    "                               text_as_string[m.start():m.end()],\n",
    "                           # Uncomment line below if you wish to have DP values alongside\n",
    "                               #\"</font><font style='vertical-align:super;font-size:8pt;color:LightGrey'>%s\" % dp_values[m.lastindex-1],\n",
    "                          \"</font>\"] )\n",
    "        i = m.end()\n",
    "    \n",
    "    html = \"\".join([output, text_as_string[m.end():], tokens_found, \"</body></html>\"])\n",
    "    \n",
    "    html = re.sub('\\n', '<br>', html)\n",
    "    \n",
    "    output_filename = text_filename.split('\\\\')[-1][:-4]+'_highlighted.html'\n",
    "    \n",
    "    with open(output_filename, 'w', encoding=chosen_encoding) as o:\n",
    "        o.write(html)\n",
    "        \n",
    "    print(datetime.now(), 'exported as file',output_filename, ' -- Process complete')\n",
    "\n",
    "    #return token_ranges\n",
    "    \n",
    "    #return difficulties, tokens, compiler, regex, html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difficulty_highlighter_latex(text_filename):\n",
    "    \n",
    "    parameter = 'latex'\n",
    "    \n",
    "    text_as_string = opentextfile(text_filename)\n",
    "    \n",
    "    text_as_string = re.sub('[“”]', '\"', text_as_string)\n",
    "    text_as_string = re.sub('[‘’]', \"'\", text_as_string)\n",
    "    text_as_string = re.sub('\\n', \"\\n\\n\", text_as_string)\n",
    "    \n",
    "    print(datetime.now(), 'text file opened')\n",
    "    \n",
    "    retrieved_tokens = calc_tds(text_as_string)\n",
    "    \n",
    "    print(datetime.now(), 'tokens retrieved')\n",
    "    \n",
    "    tds = retrieved_tokens.pop('tds', None)\n",
    "    stats = retrieved_tokens['1-grams'].pop('_stats_', None)\n",
    "    \n",
    "    difficulty_ranges = OrderedDict({'very easy': [0, 0.7],\n",
    "                                     'easy': [0.7, 0.80],\n",
    "                                     'average': [0.80, 0.90],\n",
    "                                     'slightly difficult': [0.90, 0.95],\n",
    "                                     'difficult': [0.95, 0.99],\n",
    "                                     'very difficult': [0.99, 1.00]})\n",
    "\n",
    "    parameters = OrderedDict({'latex':\n",
    "                              {'very easy': '\\\\veryeasy{',\n",
    "                               'easy': '\\\\emph{',\n",
    "                               'average': '\\\\average{',\n",
    "                               'slightly difficult': '\\\\slightlydifficult{',\n",
    "                               'difficult': '\\\\difficult{',\n",
    "                               'very difficult': '\\\\verydifficult{'},\n",
    "                               \n",
    "                               'latex_black':\n",
    "                              {'very easy': '\\\\textrm{',\n",
    "                               'easy': '\\\\emph{',\n",
    "                               'average': '\\\\underline{',\n",
    "                               'slightly difficult': '\\\\slightlydifficult{',\n",
    "                               'difficult': '\\\\difficult{',\n",
    "                               'very difficult': '\\\\verydifficult{',\n",
    "                               #'font-variant: small-caps;font-weight:bold;letter-spacing: 1.5px;color:Crimson'\n",
    "                              }})\n",
    "    \n",
    "    \n",
    "    for ngram_len in retrieved_tokens:\n",
    "        \n",
    "        for token in retrieved_tokens[ngram_len]:\n",
    "            \n",
    "            retrieved_tokens[ngram_len][token] = retrieved_tokens[ngram_len][token]['dp']\n",
    "    \n",
    "    token_ranges = {x:[] for x in difficulty_ranges}\n",
    "    \n",
    "    difficulties = []\n",
    "    tokens = []\n",
    "    dp_values = []\n",
    "    \n",
    "    for token in retrieved_tokens['1-grams']:\n",
    "        \n",
    "        token_dp = retrieved_tokens['1-grams'][token]\n",
    "        \n",
    "        for difficulty in difficulty_ranges:\n",
    "            \n",
    "            diffrange = difficulty_ranges[difficulty]\n",
    "            \n",
    "            if token_dp > diffrange[0] and token_dp <= diffrange[1]:\n",
    "                \n",
    "                difficulties.append(difficulty)\n",
    "                token_modified = \"(\" + r\"\\b\" + token + r\"\\b\" + \")\"\n",
    "                tokens.append(token_modified)\n",
    "                dp_values.append(token_dp)\n",
    "                token_ranges[difficulty].append(token)\n",
    "    \n",
    "    tokens_found = '\\n\\n\\\\section{Wordlist}\\n\\n\\\\begin{itemize}'\n",
    "    for token_range in token_ranges:\n",
    "        tokens_found+= \"\".join([\"\\n\\\\item \",\n",
    "                                \"{%s\" % parameters[parameter][token_range],\n",
    "                               token_range.capitalize(),\n",
    "                                \"}:\",\n",
    "                                \" {\\\\emph{\",\n",
    "                                ' | '.join(sorted(token_ranges[token_range])),\n",
    "                                '}}}',\n",
    "                               ])\n",
    "    tokens_found += '\\\\end{itemize}\\n'\n",
    "    \n",
    "    dp_values = [str(x)[:5] for x in dp_values]\n",
    "    \n",
    "    colors = [parameters[parameter][x] for x in difficulties]\n",
    "    \n",
    "    compiler = '|'.join(tokens)\n",
    "    regex = re.compile(compiler, re.I)\n",
    "\n",
    "    i = 0\n",
    "    output = r\"\"\"% !TEX TS-program = xelatex\n",
    "    % !TEX encoding = UTF-8\n",
    "\n",
    "    % This is a simple template for a XeLaTeX document using the \"article\" class,\n",
    "    % with the fontspec package to easily select fonts.\n",
    "\n",
    "    \\documentclass[11pt]{article} % use larger type; default would be 10pt\n",
    "\n",
    "    \\usepackage{fontspec} % Font selection for XeLaTeX; see fontspec.pdf for documentation\n",
    "    \\defaultfontfeatures{Mapping=tex-text} % to support TeX conventions like ``---''\n",
    "    \\usepackage{xunicode} % Unicode support for LaTeX character names (accents, European chars, etc)\n",
    "    \\usepackage{xltxtra} % Extra customizations for XeLaTeX\n",
    "    \\usepackage[usenames, dvipsnames]{color}\"\"\"\n",
    "    \n",
    "    if parameter == 'latex_black':\n",
    "        output += \"\"\"\n",
    "        \\definecolor{crimson}{RGB}{0,0,0}\n",
    "        \\definecolor{navy}{RGB}{0,0,0}\n",
    "        \\definecolor{lightgray}{RGB}{192,192,192}\n",
    "        \"\"\"\n",
    "        \n",
    "    else:\n",
    "        output += \"\"\"\n",
    "        \\definecolor{crimson}{RGB}{255,0,64}\n",
    "        \\definecolor{navy}{RGB}{64,0,255}\n",
    "        \\definecolor{lightgray}{RGB}{192,192,192}\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "    output += r\"\"\"\n",
    "    \n",
    "    \\setmainfont{GaramondPremrPro-Med}[\n",
    "    Extension = .otf,\n",
    "    BoldFont={GaramondPremrPro-Bd},\n",
    "    ItalicFont={GaramondPremrPro-MedIt},\n",
    "    BoldItalicFont={GaramondPremrPro-BdIt},\n",
    "    Numbers=OldStyle,\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    \\newfontfamily\\semibold{GaramondPremrPro-Smbd}\n",
    "\n",
    "    \\newfontfamily\\lightfont{GaramondPremrPro}\n",
    "\n",
    "    \\newcommand{\\verydifficult}[1] {\\begingroup\\textsc{\\textbf{\\textcolor{crimson}{#1}}}\\endgroup}\n",
    "\n",
    "    \\newcommand{\\difficult}[1] {\\begingroup\n",
    "\\lightfont{\\textsc{#1}}\\endgroup}\n",
    "\n",
    "    \\newcommand{\\slightlydifficult}[1] {\\begingroup \\textcolor{navy}{\\textbf{#1}}\\endgroup}\n",
    "    \n",
    "    \\newcommand{\\average}[1]{\\begingroup \\semibold{#1}\\endgroup}\n",
    "    \n",
    "    \\newcommand{\\veryeasy}[1]{{#1}}\n",
    "    \n",
    "    \\newcommand{\\dpsuper}[1] {\n",
    "    \\lightfont{\\textcolor{lightgray}{\\textsuperscript{#1}}}\n",
    "    }\n",
    "    \n",
    "\n",
    "    % other LaTeX packages.....\n",
    "    \\usepackage{geometry} % See geometry.pdf to learn the layout options. There are lots.\n",
    "    \\geometry{a4paper} % or letterpaper (US) or a5paper or....\n",
    "    %\\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent\n",
    "\n",
    "    \\usepackage{graphicx} % support the \\includegraphics command and options\n",
    "\n",
    "    \\title{Brief Article}\n",
    "    \\author{The Author}\n",
    "    %\\date{} % Activate to display a given date or no date (if empty),\n",
    "             % otherwise the current date is printed \n",
    "\n",
    "    \\begin{document}\n",
    "    \\maketitle\n",
    "    \"\"\"\n",
    "    \n",
    "    #output = re.sub('\\n', '', output)\n",
    "    \n",
    "    filename = text_filename.split('\\\\')[-1]\n",
    "    filename = re.sub(\"_\", '\\_', filename)\n",
    "    \n",
    "    output += filename + '\\n\\nText Difficulty Scale: \\\\textbf{'+ str(tds)[:8] + '}\\n'\n",
    "\n",
    "    legend = '\\n\\\\textbf{LEGEND ---} '\n",
    "    \n",
    "    for x in parameters[parameter]:\n",
    "        \n",
    "        legend += \"\".join([\"\",\n",
    "                           parameters[parameter][x],\n",
    "                           \"\",\n",
    "                           x.capitalize(),\n",
    "                          '} | '])\n",
    "    \n",
    "    output += \"\\n\\\\section{Highlighted Text}\" + '\\\\bigskip \\n\\n' + legend + '\\\\bigskip \\n\\n'\n",
    "    \n",
    "    print(datetime.now(), 'initiating difficulty highlighter')\n",
    "\n",
    "    for m in regex.finditer(text_as_string):\n",
    "\n",
    "        output += \"\".join([text_as_string[i:m.start()],\n",
    "                               \"%s\" % colors[m.lastindex-1],\n",
    "                               text_as_string[m.start():m.end()],\n",
    "                           # Uncomment line below if you wish to have DP values alongside\n",
    "                               \"}\\\\dpsuper{%s\" % dp_values[m.lastindex-1],\n",
    "                          \"}\"] )\n",
    "        i = m.end()\n",
    "    \n",
    "    html = \"\".join([output, text_as_string[m.end():], tokens_found, \"\\\\end{document}\"])\n",
    "    \n",
    "    output_filename = text_filename.split('\\\\')[-1][:-4] + '_highlighted.tex'\n",
    "\n",
    "    with open(output_filename, 'w', encoding=chosen_encoding) as o:\n",
    "        o.write(html)\n",
    "    \n",
    "    print(datetime.now(), 'writing to file', output_filename, ' -- Process complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-10 15:07:40.386867 text file opened\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04eb34ff0a3c46c59599e58d1a4df1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=222), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018-09-10 15:07:47.379357 tokens retrieved\n",
      "2018-09-10 15:07:49.117736 initiating difficulty highlighter\n",
      "2018-09-10 15:07:50.551873 exported as file bach_highlighted.html  -- Process complete\n"
     ]
    }
   ],
   "source": [
    "difficulty_highlighter('bach.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

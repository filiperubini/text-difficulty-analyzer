{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus builder\n",
    "Here, we will build a corpus using Pymongo as a database.\n",
    "It will install automatically the missing libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to run this notebook once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, TreebankWordTokenizer, ngrams, WhitespaceTokenizer\n",
    "from itertools import accumulate, tee, chain\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from datetime import datetime, date, time\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from collections import defaultdict, OrderedDict\n",
    "import os, os.path\n",
    "import re\n",
    "import string\n",
    "from decimal import Decimal\n",
    "from decimal import *\n",
    "getcontext().prec = 6\n",
    "import statistics\n",
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries if they're not available\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm_notebook\n",
    "except:\n",
    "    !conda install --yes --prefix {sys.prefix} tqdm\n",
    "    from tqdm import tqdm_notebook\n",
    "\n",
    "try:\n",
    "    import pymongo\n",
    "    from pymongo import MongoClient\n",
    "except:\n",
    "    !conda install --yes --prefix {sys.prefix} pymongo\n",
    "    import pymongo\n",
    "    from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_language = 'english'  # check the NLTK sentence tokenizers for available languages\n",
    "non_latin_alphabet = False # if your language uses a non-Latin alphabet, change to True\n",
    "files_dir_or_wiki = 'wiki' # change to 'list' if you have a list of filenames\n",
    "                           # or to 'dir' if you have a specific directory\n",
    "token_database = 'tokens'\n",
    "exclude_numbers = True # True / False\n",
    "exclude_numbers_for_ngrams = False # True / False\n",
    "number_of_ngrams = 1 # choose up to x number of n-grams to extract (minimum = 1)\n",
    "threads = 4 # decrease this number if you have an old/low core processor (minimum = 1)\n",
    "chosen_encoding = 'utf-8-sig' # better utf-8-sig than utf-8, saves trouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenization_language == 'japanese':\n",
    "    import tinysegmenter\n",
    "    tokenizer = tinysegmenter.TinySegmenter()\n",
    "elif non_latin_alphabet == True:\n",
    "    tokenizer = WhitespaceTokenizer()\n",
    "else:\n",
    "    tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the mongod file before running next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token statistics\n",
    "client = MongoClient()\n",
    "db = client[token_database]\n",
    "\n",
    "token_stats = db[token_database]\n",
    "token_stats.allowDiskUse=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try getting our text data (if it already exists)\n",
    "try:\n",
    "    text_stats = token_stats.find_one({'_text-stats': True})['text_stats']\n",
    "except:\n",
    "    text_stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete DB and Indexes, if anything goes wrong\n",
    "#token_stats.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indexes for faster updates / retrievals\n",
    "for value in ['token', 'disp', 'freq', 'len', 'occurred_in', 'dp', '_text-stats']:\n",
    "    index = token_stats.create_index([(value, pymongo.ASCENDING)])\n",
    "\n",
    "index_occurred_len = token_stats.create_index([('occurred_in', pymongo.ASCENDING),\n",
    "                                              ('len', pymongo.ASCENDING)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number_repl_isdigit(s):\n",
    "    '''Returns True is string is a number,\n",
    "    i.e. if it contains dot or comma.'''\n",
    "    \n",
    "    return re.sub('[.,]', '', s).isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if non_latin_alphabet == False:\n",
    "\n",
    "    def cleantokensfromsentence(tokens, bar_numbers):\n",
    "        '''Clean a list of tokens to remove\n",
    "        extraneous characters and numerals.\n",
    "        Bar_numbers: if True, numbers will be deleted'''\n",
    "\n",
    "        clean_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "\n",
    "            if is_number_repl_isdigit(token) == True:\n",
    "                if bar_numbers == True:\n",
    "                    '''Barring numbers'''\n",
    "                    pass\n",
    "                elif bar_numbers == False:\n",
    "                    clean_tokens.append(token)\n",
    "            elif token.isalnum() is False:\n",
    "                if len(token) == 1:\n",
    "                    pass\n",
    "                else:\n",
    "                    matches = 0\n",
    "                    for character in token:\n",
    "                        if character.isalnum() is False:\n",
    "                            matches += 1\n",
    "                    if matches == len(token):\n",
    "                        pass\n",
    "                    else:\n",
    "                        clean_tokens.append(token)\n",
    "            else:\n",
    "                clean_tokens.append(token)\n",
    "\n",
    "        return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if non_latin_alphabet == True:\n",
    "    \n",
    "    def cleantokensfromsentence(tokens, bar_numbers):\n",
    "        '''Clean a list of tokens to remove numerals.s'''\n",
    "\n",
    "        clean_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "\n",
    "            if is_number_repl_isdigit(token) == True:\n",
    "                if bar_numbers == True:\n",
    "                    '''Barring numbers'''\n",
    "                    pass\n",
    "                elif bar_numbers == False:\n",
    "                    clean_tokens.append(token)\n",
    "            else:\n",
    "                clean_tokens.append(token)\n",
    "\n",
    "        return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmultitokens(sentence, n_of_ngrams):\n",
    "    '''From a list of tokens, generate up to n n-grams'''\n",
    "    \n",
    "    all_ngrams = []\n",
    "    \n",
    "    for x in range(2, n_of_ngrams+1):\n",
    "        all_ngrams.extend(ngrams(sentence, x))\n",
    "        \n",
    "    return [' '.join(i) for i in list(chain(all_ngrams))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opentextfile(text_filename):\n",
    "    global chosen_encoding\n",
    "    with open(text_filename, 'r', encoding=chosen_encoding) as t:\n",
    "        return t.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "    \n",
    "    global number_of_ngrams, tokenization_language\n",
    "    global exclude_numbers, exclude_numbers_for_ngrams\n",
    "\n",
    "    tempdict = {}\n",
    "\n",
    "    # Barring numbers on single tokens\n",
    "    \n",
    "    if tokenization_language == 'japanese':\n",
    "        global segmenter\n",
    "        original_tokens = segmenter.tokenize(sentence)\n",
    "    else:\n",
    "        original_tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    for token in cleantokensfromsentence(original_tokens,exclude_numbers):\n",
    "        addtodict(token, {'token': token,\n",
    "                         'freq': 1}, tempdict)\n",
    "\n",
    "    \n",
    "    if number_of_ngrams > 1:\n",
    "\n",
    "        for token in getmultitokens(cleantokensfromsentence(original_tokens,exclude_numbers_for_ngrams), number_of_ngrams):\n",
    "            addtodict(token, {'token': token,\n",
    "                             'freq': 1}, tempdict)\n",
    "    \n",
    "    del original_tokens\n",
    "\n",
    "    return tempdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addtodict(key, value, data_dict):\n",
    "    '''Update a dictionary with a key and value.\n",
    "    If key exists, combines the values.'''\n",
    "    \n",
    "    if type(value) == int:\n",
    "        # Sums existing value to current value\n",
    "        new_value = data_dict.get(key, 0) + value\n",
    "    \n",
    "    elif type(value) == str:\n",
    "        # Keep existing value\n",
    "        new_value = data_dict.get(key, value)\n",
    "    \n",
    "    elif type(value) == dict:\n",
    "        new_value = data_dict.get(key, {})\n",
    "        \n",
    "        for d1_key in value:\n",
    "            addtodict(d1_key, value[d1_key], new_value)\n",
    "\n",
    "    data_dict[key] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textdata(text_filename):\n",
    "    \n",
    "    global tokenization_language, non_latin_alphabet\n",
    "    \n",
    "    token_data = {}\n",
    "    \n",
    "    text_as_string = opentextfile(text_filename)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "\n",
    "    textlc = text_as_string.lower()\n",
    "    del text_as_string\n",
    "    \n",
    "    # Uncomment this next line of code\n",
    "    # if you are working with a Wikipedia dump file\n",
    "    \n",
    "    #textlc = textlc.split('\">\\n')[1]\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    sentences = sent_tokenize(textlc, language=tokenization_language)\n",
    "    \n",
    "    del textlc\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        results.append(process_sentence(sentence))\n",
    "        \n",
    "    for tempdict in results:\n",
    "        \n",
    "        for key in tempdict:\n",
    "            addtodict(key, tempdict[key], token_data)\n",
    "\n",
    "    return list(token_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_token(token_entry):\n",
    "    \n",
    "    sub_entry = 'freq_in_file' + '.' + str(token_entry['file_id'])\n",
    "    \n",
    "    result = token_stats.update_one({'token': token_entry['token']},\n",
    "                                        {'$inc': {'freq': token_entry['freq'],\n",
    "                                                 'disp': 1},\n",
    "                                         '$setOnInsert': {'len': len(token_entry['token'].split(' '))},\n",
    "                                         '$push': {'occurred_in': str(token_entry['file_id']),\n",
    "                                                  'freq_occurred_in': token_entry['freq']}},\n",
    "                                    upsert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db(file_id):\n",
    "    \n",
    "    global token_stats, file_dict\n",
    "\n",
    "    text_filename = file_dict[file_id]\n",
    "    \n",
    "    token_entries = textdata(text_filename)\n",
    "\n",
    "    for token_entry in token_entries:\n",
    "        token_entry['file_id'] = file_id\n",
    "    \n",
    "    pool = ThreadPool(threads)\n",
    "    results = pool.map(update_token, token_entries)\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_freq(file_id):\n",
    "    \n",
    "    global text_stats, token_stats, n_of_text_files, step, ngramrange\n",
    "    \n",
    "    for n_of_ngrams in ngramrange:\n",
    "\n",
    "        text_freq = token_stats.aggregate([{'$match': \n",
    "                                            {'occurred_in': file_id,\n",
    "                                             'len': n_of_ngrams}},\n",
    "                                           {'$group': {'_id': None,\n",
    "                                        \n",
    "                                        'freq': \n",
    "                                          {'$sum': {'$arrayElemAt': \n",
    "                                           ['$freq_occurred_in', \n",
    "                                            {'$indexOfArray':\n",
    "                                             ['$occurred_in', file_id]}]}}}}])\n",
    "\n",
    "        for result in text_freq:\n",
    "            text_stats['total'][str(n_of_ngrams)+'-grams'][file_id] = result['freq']\n",
    "\n",
    "#     n_of_text_files += 1\n",
    "    \n",
    "#     if n_of_text_files % step == 0:\n",
    "#         print(datetime.now(), n_of_text_files, 'text files processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text file location</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if files_dir_or_wiki == 'list':\n",
    "    # Change to the list of filenames of your choice\n",
    "    list_of_files_path = 'chosen_articles.txt'\n",
    "    text_files = open(list_of_files_path, 'r', encoding=chosen_encoding)\n",
    "    text_filenames = [x[:-1] for x in text_files.readlines()]\n",
    "\n",
    "elif files_dir_or_wiki == 'dir':\n",
    "    # Change to the text file directory of your choice\n",
    "    text_file_directory = '.\\\\chosen_articles\\\\'\n",
    "    text_filenames = [os.path.join(text_file_directory,f.name) for f in os.scandir(text_file_directory) if f.is_file()]\n",
    "\n",
    "elif files_dir_or_wiki == 'wiki':\n",
    "    # Point to Wikipedia extracted article directory\n",
    "    text_file_directory = '.\\\\output_dir\\\\'\n",
    "    text_filenames = [os.path.join(text_file_directory,f.name) for f in os.scandir(text_file_directory) if f.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of filenames\n",
    "# in order to generate number-filename \"code\"\n",
    "\n",
    "f_n = 0\n",
    "file_dict = {}\n",
    "for filename in text_filenames:\n",
    "    file_dict[str(f_n)] = filename\n",
    "    f_n += 1\n",
    "del text_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating corpus parts\n",
    "\n",
    "if number_of_ngrams <= 1:\n",
    "    ngramrange = [1]\n",
    "elif number_of_ngrams > 1:\n",
    "    ngramrange = range(1,number_of_ngrams+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you will build your corpus by extracting basic data\n",
    "(frequency, dispersion, the frequency of tokens in a given text, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files already processed\n",
      "100 files remaining with a step of 2 , half: 50\n"
     ]
    }
   ],
   "source": [
    "already_processed = token_stats.distinct('occurred_in')\n",
    "print(len(already_processed), 'files already processed')\n",
    "\n",
    "for file_id in already_processed:\n",
    "    del file_dict[file_id]\n",
    "\n",
    "n_files = 0\n",
    "step = ((len(file_dict)) // 100) + 1\n",
    "half = len(file_dict) // 2\n",
    "print(len(file_dict), 'files remaining with a step of', step, ', half:', half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-10 14:17:31.329844 started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d1f95c76194be2940354ff60f63c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018-09-10 14:17:58.357932 ended\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now(), 'started')\n",
    "for file_id in tqdm_notebook(list(file_dict.keys())):\n",
    "    update_db(file_id)\n",
    "print(datetime.now(), 'ended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Obtain Text Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prerequisite for calculating DP in the next Part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871307c3e3af4f85bb45a67224c3b95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be90f0076ba34d419d90d612e206e1d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating corpus parts\n",
    "\n",
    "file_ids = token_stats.distinct('occurred_in')\n",
    "\n",
    "text_stats = {'total': {}}\n",
    "\n",
    "for key in tqdm_notebook(text_stats):\n",
    "\n",
    "    for n_of_ngrams in tqdm_notebook(ngramrange):\n",
    "        text_stats[key][str(n_of_ngrams)+'-grams'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-10 14:18:27.037231 started calculating overall frequencies\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbce85032504cde9888df8fd92dea0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018-09-10 14:18:33.353892 ended calculating overall frequencies\n"
     ]
    }
   ],
   "source": [
    "n_of_text_files = 0\n",
    "step = len(file_ids) // 1000\n",
    "half = len(file_ids) // 2\n",
    "\n",
    "print(datetime.now(), 'started calculating overall frequencies')\n",
    "\n",
    "pool = ThreadPool(threads)\n",
    "results = pool.map(get_text_freq, tqdm_notebook(file_ids))\n",
    "pool.close()\n",
    "        \n",
    "print(datetime.now(), 'ended calculating overall frequencies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating normalized frequencies\n",
    "\n",
    "for token_length in text_stats['total']:\n",
    "    overall_freq = sum(text_stats['total'][token_length].values())\n",
    "    \n",
    "    for file_id in text_stats['total'][token_length]:\n",
    "        absfreq = text_stats['total'][token_length][file_id]\n",
    "        relfreq = Decimal(absfreq) / Decimal(overall_freq)\n",
    "        text_stats['total'][token_length][file_id] = {'absfreq': absfreq,\n",
    "                                                     'relfreq': float(relfreq)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_stats_insert = token_stats.insert_one({'_text-stats': True,\n",
    "                                  'text_stats': text_stats})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_stats = token_stats.find_one({'_text-stats': True})['text_stats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Calculate DP by Gries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Stephan Th. Gries's (2008, 2010) \"deviation of proportions\", in order to see how well dispersed tokens are in your corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dp(token):\n",
    "    \n",
    "    global text_stats, token_stats\n",
    "    \n",
    "    try:\n",
    "        # See if DP has already been calculated\n",
    "        dp = token_stats.find_one({'token': token})['dp']\n",
    "        return dp\n",
    "    \n",
    "    except TypeError:\n",
    "        # Token not found in DB\n",
    "        return float(1)\n",
    "    \n",
    "    except KeyError:\n",
    "        # DP was not calculated yet\n",
    "        # Let's calculate it then\n",
    "\n",
    "        try:\n",
    "            document = token_stats.find_one({'token': token})\n",
    "            token_length = document['len']\n",
    "            token_freq = document['freq']\n",
    "\n",
    "            freq_in_files = dict(zip(document['occurred_in'],document['freq_occurred_in']))\n",
    "\n",
    "            for file_id in freq_in_files:\n",
    "                freq_in_files[file_id] = Decimal(freq_in_files[file_id]) / Decimal(token_freq)\n",
    "\n",
    "            differences = float(0)\n",
    "\n",
    "            for file_id in text_stats['total'][str(token_length)+'-grams']:\n",
    "\n",
    "                expected_percentage = text_stats['total'][str(token_length)+'-grams'][file_id]['relfreq']\n",
    "                expected_percentage = abs(float(expected_percentage))\n",
    "                observed_percentage = abs(float(freq_in_files.get(file_id, float(0))))\n",
    "\n",
    "                diff = abs(expected_percentage - observed_percentage)\n",
    "                differences += diff\n",
    "            \n",
    "            dp = Decimal(differences) / Decimal(2)\n",
    "            \n",
    "            # Before returning DP, let us insert it to the DB\n",
    "            \n",
    "            result = token_stats.update_one({'token': token},\n",
    "                                    {'$set': {'dp': float(dp)}})\n",
    "\n",
    "            return float(dp)\n",
    "\n",
    "        except TypeError:\n",
    "            # Token not found in DB\n",
    "            return float(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this operation fails,\n",
    "# try dividing the MongoDB request more.\n",
    "# For instance:\n",
    "# first you run this cell for tokens with more than 1,000 occurrences\n",
    "# all_tokens = token_stats.distinct('token', {'freq': {'$gt': 1000}})\n",
    "# then you run this cell for tokens with less than 1,000 occurrences\n",
    "# all_tokens = token_stats.distinct('token', {'freq': {'$lt': 1000})\n",
    "# It will all depend on the size of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-10 14:18:55.107166 adding DP values for 9661 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f354efd838be4ee0a6068bca037db807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9661), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018-09-10 14:19:08.619131 finished adding DP values for 9661 tokens\n"
     ]
    }
   ],
   "source": [
    "all_tokens = token_stats.distinct('token', {'dp': {'$exists': False}})\n",
    "tokens_processed = 0\n",
    "\n",
    "print(datetime.now(), 'adding DP values for', len(all_tokens), 'tokens')\n",
    "pool = ThreadPool(threads)\n",
    "results = pool.map(calc_dp, tqdm_notebook(all_tokens))\n",
    "pool.close()\n",
    "print(datetime.now(), 'finished adding DP values for', len(all_tokens), 'tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

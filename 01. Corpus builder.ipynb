{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus builder\n",
    "Here, we will build a corpus using Pymongo as a database.\n",
    "It will install automatically the missing libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to run this notebook once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, TreebankWordTokenizer, ngrams, WhitespaceTokenizer\n",
    "from itertools import accumulate, tee, chain\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from datetime import datetime, date, time\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from collections import defaultdict, OrderedDict\n",
    "import os, os.path\n",
    "import re\n",
    "import string\n",
    "from decimal import Decimal\n",
    "from decimal import *\n",
    "getcontext().prec = 6\n",
    "import statistics\n",
    "from IPython.display import display, Markdown, Latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries if they're not available\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm_notebook\n",
    "except:\n",
    "    try:\n",
    "        !conda install --yes --prefix {sys.prefix} tqdm\n",
    "    except:\n",
    "        !{sys.executable} -m pip install tqdm\n",
    "\n",
    "try:\n",
    "    import pymongo\n",
    "    from pymongo import MongoClient\n",
    "except:\n",
    "    try:\n",
    "        !conda install --yes --prefix {sys.prefix} pymongo\n",
    "    except:\n",
    "        !{sys.executable} -m pip install pymongo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization_language = 'persian'  # check the NLTK sentence tokenizers for available languages\n",
    "non_latin_alphabet = True # if your language uses a non-Latin alphabet, change to True\n",
    "files_dir_or_wiki = 'wiki' # change to 'list' if you have a list of filenames\n",
    "                           # or to 'dir' if you have a specific directory\n",
    "token_database = 'tokens-persian'\n",
    "exclude_numbers = True # True / False\n",
    "exclude_numbers_for_ngrams = False # True / False\n",
    "number_of_ngrams = 1 # choose up to x number of n-grams to extract (minimum = 1)\n",
    "threads = 4 # decrease this number if you have an old/low core processor (minimum = 1)\n",
    "chosen_encoding = 'utf-8-sig' # better utf-8-sig than utf-8, saves trouble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filipe/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Module 'PyICU' is deprecated, import 'icu' instead\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "if non_latin_alphabet == False:\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "elif non_latin_alphabet == True:\n",
    "    \n",
    "    try:\n",
    "        from polyglot.downloader import downloader\n",
    "    except ModuleNotFoundError:\n",
    "        !{sys.executable} -m pip install polyglot\n",
    "    \n",
    "    try:\n",
    "        import PyICU, icu\n",
    "    except ModuleNotFoundError:\n",
    "        !{sys.executable} -m pip install PyICU\n",
    "    \n",
    "    try:\n",
    "        import pycld2\n",
    "    except ModuleNotFoundError:\n",
    "        !{sys.executable} -m pip install pycld2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing language-specific module\n",
    "\n",
    "from polyglot.downloader import downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ICU-supported languages\n",
    "\n",
    "langs = {'afrikaans': 'af',\n",
    " 'alemannic': 'als',\n",
    " 'amharic': 'am',\n",
    " 'aragonese': 'an',\n",
    " 'arabic': 'ar',\n",
    " 'egyptian arabic': 'arz',\n",
    " 'assamese': 'as',\n",
    " 'asturian': 'ast',\n",
    " 'azerbaijani': 'az',\n",
    " 'bashkir': 'ba',\n",
    " 'bavarian': 'bar',\n",
    " 'belarusian': 'be',\n",
    " 'bulgarian': 'bg',\n",
    " 'bangla': 'bn',\n",
    " 'tibetan': 'bo',\n",
    " 'bishnupriya': 'bpy',\n",
    " 'breton': 'br',\n",
    " 'bosnian': 'bs',\n",
    " 'catalan': 'ca',\n",
    " 'chechen': 'ce',\n",
    " 'cebuano': 'ceb',\n",
    " 'czech': 'cs',\n",
    " 'chuvash': 'cv',\n",
    " 'welsh': 'cy',\n",
    " 'danish': 'da',\n",
    " 'german': 'de',\n",
    " 'zazaki': 'diq',\n",
    " 'divehi': 'dv',\n",
    " 'greek': 'el',\n",
    " 'english': 'en',\n",
    " 'esperanto': 'eo',\n",
    " 'spanish': 'es',\n",
    " 'estonian': 'et',\n",
    " 'basque': 'eu',\n",
    " 'persian': 'fa',\n",
    " 'finnish': 'fi',\n",
    " 'faroese': 'fo',\n",
    " 'french': 'fr',\n",
    " 'western frisian': 'fy',\n",
    " 'irish': 'ga',\n",
    " 'gan chinese': 'gan',\n",
    " 'scottish gaelic': 'gd',\n",
    " 'galician': 'gl',\n",
    " 'gujarati': 'gu',\n",
    " 'manx': 'gv',\n",
    " 'hebrew': 'he',\n",
    " 'hindi': 'hi',\n",
    " 'fiji hindi': 'hif',\n",
    " 'croatian': 'hr',\n",
    " 'upper sorbian': 'hsb',\n",
    " 'haitian creole': 'ht',\n",
    " 'hungarian': 'hu',\n",
    " 'armenian': 'hy',\n",
    " 'interlingua': 'ia',\n",
    " 'indonesian': 'id',\n",
    " 'iloko': 'ilo',\n",
    " 'ido': 'io',\n",
    " 'icelandic': 'is',\n",
    " 'italian': 'it',\n",
    " 'japanese': 'ja',\n",
    " 'javanese': 'jv',\n",
    " 'georgian': 'ka',\n",
    " 'kazakh': 'kk',\n",
    " 'khmer': 'km',\n",
    " 'kannada': 'kn',\n",
    " 'korean': 'ko',\n",
    " 'kurdish': 'ku',\n",
    " 'kyrgyz': 'ky',\n",
    " 'latin': 'la',\n",
    " 'luxembourgish': 'lb',\n",
    " 'limburgish': 'li',\n",
    " 'lombard': 'lmo',\n",
    " 'lithuanian': 'lt',\n",
    " 'latvian': 'lv',\n",
    " 'malagasy': 'mg',\n",
    " 'macedonian': 'mk',\n",
    " 'malayalam': 'ml',\n",
    " 'mongolian': 'mn',\n",
    " 'marathi': 'mr',\n",
    " 'malay': 'ms',\n",
    " 'maltese': 'mt',\n",
    " 'burmese': 'my',\n",
    " 'nepali': 'ne',\n",
    " 'dutch': 'nl',\n",
    " 'norwegian nynorsk': 'nn',\n",
    " 'norwegian': 'no',\n",
    " 'occitan': 'oc',\n",
    " 'odia': 'or',\n",
    " 'ossetic': 'os',\n",
    " 'punjabi': 'pa',\n",
    " 'pampanga': 'pam',\n",
    " 'polish': 'pl',\n",
    " 'piedmontese': 'pms',\n",
    " 'pashto': 'ps',\n",
    " 'portuguese': 'pt',\n",
    " 'quechua': 'qu',\n",
    " 'romansh': 'rm',\n",
    " 'romanian': 'ro',\n",
    " 'russian': 'ru',\n",
    " 'sanskrit': 'sa',\n",
    " 'sakha': 'sah',\n",
    " 'sicilian': 'scn',\n",
    " 'scots': 'sco',\n",
    " 'northern sami': 'se',\n",
    " 'serbo-croatian': 'sh',\n",
    " 'sinhala': 'si',\n",
    " 'slovak': 'sk',\n",
    " 'slovenian': 'sl',\n",
    " 'albanian': 'sq',\n",
    " 'serbian': 'sr',\n",
    " 'sundanese': 'su',\n",
    " 'swedish': 'sv',\n",
    " 'swahili': 'sw',\n",
    " 'silesian': 'szl',\n",
    " 'tamil': 'ta',\n",
    " 'telugu': 'te',\n",
    " 'tajik': 'tg',\n",
    " 'thai': 'th',\n",
    " 'turkmen': 'tk',\n",
    " 'tagalog': 'tl',\n",
    " 'turkish': 'tr',\n",
    " 'tatar': 'tt',\n",
    " 'uyghur': 'ug',\n",
    " 'ukrainian': 'uk',\n",
    " 'urdu': 'ur',\n",
    " 'uzbek': 'uz',\n",
    " 'venetian': 'vec',\n",
    " 'vietnamese': 'vi',\n",
    " 'west flemish': 'vls',\n",
    " 'volap√ºk': 'vo',\n",
    " 'walloon': 'wa',\n",
    " 'waray': 'war',\n",
    " 'yiddish': 'yi',\n",
    " 'yoruba': 'yo',\n",
    " 'chinese': 'zh',\n",
    " 'chinese character': 'zhc',\n",
    " 'chinese word': 'zhw'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persian supported with models: ['fa']\n"
     ]
    }
   ],
   "source": [
    "if tokenization_language.lower() in langs:\n",
    "    \n",
    "    token_models = []\n",
    "    \n",
    "    for lang in langs:\n",
    "        code = langs[lang]\n",
    "        \n",
    "        if tokenization_language in lang:\n",
    "            token_models.append(code)\n",
    "    \n",
    "    print(tokenization_language.capitalize(), 'supported with models:', token_models)\n",
    "\n",
    "elif tokenization_language.lower() not in langs:\n",
    "    print(tokenization_language.capitalize(), 'not supported. Reverting to Treebank')\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyglot_data] Downloading collection 'LANG:fa'\n",
      "[polyglot_data]    | \n",
      "[polyglot_data]    | Downloading package sgns2.fa to\n",
      "[polyglot_data]    |     /home/filipe/polyglot_data...\n",
      "[polyglot_data]    | Downloading package unipos.fa to\n",
      "[polyglot_data]    |     /home/filipe/polyglot_data...\n",
      "[polyglot_data]    | Downloading package ner2.fa to\n",
      "[polyglot_data]    |     /home/filipe/polyglot_data...\n",
      "[polyglot_data]    | Downloading package counts2.fa to\n",
      "[polyglot_data]    |     /home/filipe/polyglot_data...\n",
      "[polyglot_data]    | Downloading package transliteration2.fa to\n",
      "[polyglot_data]    |     /home/filipe/polyglot_data...\n",
      "[polyglot_data]    | Downloading package embeddings2.fa to\n",
      "[polyglot_data]    |     /home/filipe/polyglot_data...\n",
      "[polyglot_data]    | Downloading package uniemb.fa to\n",
      "[polyglot_data]    |     /home/filipe/polyglot_data...\n",
      "[polyglot_data]    | Downloading package sentiment2.fa to\n",
      "[polyglot_data]    |     /home/filipe/polyglot_data...\n",
      "[polyglot_data]    | Downloading package tsne2.fa to\n",
      "[polyglot_data]    |     /home/filipe/polyglot_data...\n",
      "[polyglot_data]    | Downloading package morph2.fa to\n",
      "[polyglot_data]    |     /home/filipe/polyglot_data...\n",
      "[polyglot_data]    | \n",
      "[polyglot_data]  Done downloading collection LANG:fa\n"
     ]
    }
   ],
   "source": [
    "# Downloading polyglot language models for the language you've chosen\n",
    "\n",
    "for model in token_models:\n",
    "    \n",
    "    downloader.download('LANG:'+model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from polyglot.text import Text\n",
    "except ModuleNotFoundError:\n",
    "    !{sys.executable} -m pip install morfessor\n",
    "\n",
    "from polyglot.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def icu_tokenizer(text_as_string):\n",
    "    \n",
    "    global token_models\n",
    "    \n",
    "    text = Text(text_as_string)\n",
    "    text.language = token_models[0]\n",
    "    \n",
    "    return [str(word) for word in text.words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def icu_sentence_tokenizer(text_as_string):\n",
    "    \n",
    "    global token_models\n",
    "    \n",
    "    text = Text(text_as_string)\n",
    "    text.language = token_models[0]\n",
    "    \n",
    "    return [str(sentence) for sentence in text.sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the mongod file before running next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token statistics\n",
    "client = MongoClient()\n",
    "db = client[token_database]\n",
    "\n",
    "token_stats = db[token_database]\n",
    "token_stats.allowDiskUse=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try getting our text data (if it already exists)\n",
    "try:\n",
    "    text_stats = token_stats.find_one({'_text-stats': True})['text_stats']\n",
    "except:\n",
    "    text_stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete DB and Indexes, if anything goes wrong\n",
    "#token_stats.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create indexes for faster updates / retrievals\n",
    "for value in ['token', 'disp', 'freq', 'len', 'occurred_in', 'dp', '_text-stats']:\n",
    "    index = token_stats.create_index([(value, pymongo.ASCENDING)])\n",
    "\n",
    "index_occurred_len = token_stats.create_index([('occurred_in', pymongo.ASCENDING),\n",
    "                                              ('len', pymongo.ASCENDING)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number_repl_isdigit(s):\n",
    "    '''Returns True is string is a number,\n",
    "    i.e. if it contains dot or comma.'''\n",
    "    \n",
    "    return re.sub('[.,]', '', s).isdigit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleantokensfromsentence(tokens, bar_numbers):\n",
    "    '''Clean a list of tokens to remove\n",
    "    extraneous characters and numerals.\n",
    "    Bar_numbers: if True, numbers will be deleted'''\n",
    "\n",
    "    clean_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "\n",
    "        if is_number_repl_isdigit(token) == True:\n",
    "            if bar_numbers == True:\n",
    "                '''Barring numbers'''\n",
    "                pass\n",
    "            elif bar_numbers == False:\n",
    "                clean_tokens.append(token)\n",
    "        elif token.isalnum() is False:\n",
    "            if len(token) == 1:\n",
    "                pass\n",
    "            else:\n",
    "                matches = 0\n",
    "                for character in token:\n",
    "                    if character.isalnum() is False:\n",
    "                        matches += 1\n",
    "                if matches == len(token):\n",
    "                    pass\n",
    "                else:\n",
    "                    clean_tokens.append(token)\n",
    "        else:\n",
    "            clean_tokens.append(token)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmultitokens(sentence, n_of_ngrams):\n",
    "    '''From a list of tokens, generate up to n n-grams'''\n",
    "    \n",
    "    all_ngrams = []\n",
    "    \n",
    "    for x in range(2, n_of_ngrams+1):\n",
    "        all_ngrams.extend(ngrams(sentence, x))\n",
    "        \n",
    "    return [' '.join(i) for i in list(chain(all_ngrams))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opentextfile(text_filename):\n",
    "    global chosen_encoding\n",
    "    with open(text_filename, 'r', encoding=chosen_encoding) as t:\n",
    "        return t.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addtodict(key, value, data_dict):\n",
    "    '''Update a dictionary with a key and value.\n",
    "    If key exists, combines the values.'''\n",
    "    \n",
    "    if type(value) == int:\n",
    "        # Sums existing value to current value\n",
    "        new_value = data_dict.get(key, 0) + value\n",
    "    \n",
    "    elif type(value) == str:\n",
    "        # Keep existing value\n",
    "        new_value = data_dict.get(key, value)\n",
    "    \n",
    "    elif type(value) == dict:\n",
    "        new_value = data_dict.get(key, {})\n",
    "        \n",
    "        for d1_key in value:\n",
    "            addtodict(d1_key, value[d1_key], new_value)\n",
    "\n",
    "    data_dict[key] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentence(sentence):\n",
    "    \n",
    "    global number_of_ngrams, non_latin_alphabet\n",
    "    global exclude_numbers, exclude_numbers_for_ngrams\n",
    "\n",
    "    tempdict = {}\n",
    "\n",
    "    # Barring numbers on single tokens\n",
    "    \n",
    "    if non_latin_alphabet == False:\n",
    "        original_tokens = tokenizer.tokenize(sentence)\n",
    "    elif non_latin_alphabet == True:\n",
    "        original_tokens = icu_tokenizer(sentence)\n",
    "    \n",
    "    for token in cleantokensfromsentence(original_tokens,exclude_numbers):\n",
    "        addtodict(token, {'token': token,\n",
    "                         'freq': 1}, tempdict)\n",
    "    \n",
    "    if number_of_ngrams > 1:\n",
    "\n",
    "        for token in getmultitokens(cleantokensfromsentence(original_tokens,exclude_numbers_for_ngrams), number_of_ngrams):\n",
    "            addtodict(token, {'token': token,\n",
    "                             'freq': 1}, tempdict)\n",
    "    \n",
    "    del original_tokens\n",
    "\n",
    "    return tempdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textdata(text_filename):\n",
    "    \n",
    "    global tokenization_language, non_latin_alphabet\n",
    "    \n",
    "    token_data = {}\n",
    "    \n",
    "    text_as_string = opentextfile(text_filename)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "\n",
    "    textlc = text_as_string.lower()\n",
    "    del text_as_string\n",
    "    \n",
    "    # Uncomment this next line of code\n",
    "    # if you are working with a Wikipedia dump file\n",
    "    \n",
    "    #textlc = textlc.split('\">\\n')[1]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    if non_latin_alphabet == False:\n",
    "        sentences = sent_tokenize(textlc, language=tokenization_language)\n",
    "    elif non_latin_alphabet == True:\n",
    "        sentences = icu_sentence_tokenizer(textlc)\n",
    "    \n",
    "    del textlc\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        results.append(process_sentence(sentence))\n",
    "        \n",
    "    for tempdict in results:\n",
    "        \n",
    "        for key in tempdict:\n",
    "            addtodict(key, tempdict[key], token_data)\n",
    "\n",
    "    return list(token_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_token(token_entry):\n",
    "    \n",
    "    sub_entry = 'freq_in_file' + '.' + str(token_entry['file_id'])\n",
    "    \n",
    "    result = token_stats.update_one({'token': token_entry['token']},\n",
    "                                        {'$inc': {'freq': token_entry['freq'],\n",
    "                                                 'disp': 1},\n",
    "                                         '$setOnInsert': {'len': len(token_entry['token'].split(' '))},\n",
    "                                         '$push': {'occurred_in': str(token_entry['file_id']),\n",
    "                                                  'freq_occurred_in': token_entry['freq']}},\n",
    "                                    upsert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_db(file_id):\n",
    "    \n",
    "    global token_stats, file_dict\n",
    "\n",
    "    text_filename = file_dict[file_id]\n",
    "    \n",
    "    token_entries = textdata(text_filename)\n",
    "\n",
    "    for token_entry in token_entries:\n",
    "        token_entry['file_id'] = file_id\n",
    "    \n",
    "    pool = ThreadPool(threads)\n",
    "    results = pool.map(update_token, token_entries)\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_freq(file_id):\n",
    "    \n",
    "    global text_stats, token_stats, n_of_text_files, step, ngramrange\n",
    "    \n",
    "    for n_of_ngrams in ngramrange:\n",
    "\n",
    "        text_freq = token_stats.aggregate([{'$match': \n",
    "                                            {'occurred_in': file_id,\n",
    "                                             'len': n_of_ngrams}},\n",
    "                                           {'$group': {'_id': None,\n",
    "                                        \n",
    "                                        'freq': \n",
    "                                          {'$sum': {'$arrayElemAt': \n",
    "                                           ['$freq_occurred_in', \n",
    "                                            {'$indexOfArray':\n",
    "                                             ['$occurred_in', file_id]}]}}}}])\n",
    "\n",
    "        for result in text_freq:\n",
    "            text_stats['total'][str(n_of_ngrams)+'-grams'][file_id] = result['freq']\n",
    "\n",
    "#     n_of_text_files += 1\n",
    "    \n",
    "#     if n_of_text_files % step == 0:\n",
    "#         print(datetime.now(), n_of_text_files, 'text files processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text file location</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if files_dir_or_wiki == 'list':\n",
    "    # Change to the list of filenames of your choice\n",
    "    list_of_files_path = 'chosen_articles.txt'\n",
    "    text_files = open(list_of_files_path, 'r', encoding=chosen_encoding)\n",
    "    text_filenames = [x[:-1] for x in text_files.readlines()]\n",
    "\n",
    "elif files_dir_or_wiki == 'dir':\n",
    "    # Change to the text file directory of your choice\n",
    "    text_file_directory = './/chosen_articles//'\n",
    "    text_filenames = [os.path.join(text_file_directory,f.name) for f in os.scandir(text_file_directory) if f.is_file()]\n",
    "\n",
    "elif files_dir_or_wiki == 'wiki':\n",
    "    # Point to Wikipedia extracted article directory\n",
    "    text_file_directory = './/output_dir//'\n",
    "    text_filenames = [os.path.join(text_file_directory,f.name) for f in os.scandir(text_file_directory) if f.is_file()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of filenames\n",
    "# in order to generate number-filename \"code\"\n",
    "\n",
    "f_n = 0\n",
    "file_dict = {}\n",
    "for filename in text_filenames:\n",
    "    file_dict[str(f_n)] = filename\n",
    "    f_n += 1\n",
    "del text_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating corpus parts\n",
    "\n",
    "if number_of_ngrams <= 1:\n",
    "    ngramrange = [1]\n",
    "elif number_of_ngrams > 1:\n",
    "    ngramrange = range(1,number_of_ngrams+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you will build your corpus by extracting basic data\n",
    "(frequency, dispersion, the frequency of tokens in a given text, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 files already processed\n",
      "2627 files remaining with a step of 27 , half: 1313\n"
     ]
    }
   ],
   "source": [
    "already_processed = token_stats.distinct('occurred_in')\n",
    "print(len(already_processed), 'files already processed')\n",
    "\n",
    "for file_id in already_processed:\n",
    "    del file_dict[file_id]\n",
    "\n",
    "n_files = 0\n",
    "step = ((len(file_dict)) // 100) + 1\n",
    "half = len(file_dict) // 2\n",
    "print(len(file_dict), 'files remaining with a step of', step, ', half:', half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-13 07:23:04.148056 started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf34b1ed4ef411e8a6bd49721aa2c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2627), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n",
      "Detector is not able to detect the language reliably.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018-09-13 07:31:20.707760 ended\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(datetime.now(), 'started')\n",
    "    for file_id in tqdm_notebook(list(file_dict.keys())):\n",
    "        update_db(file_id)\n",
    "    print(datetime.now(), 'ended')\n",
    "except LookupError:\n",
    "    import nltk\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Obtain Text Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prerequisite for calculating DP in the next Part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4a387584b84c5c8e61fa9e8f1f049e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c69d7d1bb847eb82d6a0357c18fdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating corpus parts\n",
    "\n",
    "file_ids = token_stats.distinct('occurred_in')\n",
    "\n",
    "text_stats = {'total': {}}\n",
    "\n",
    "for key in tqdm_notebook(text_stats):\n",
    "\n",
    "    for n_of_ngrams in tqdm_notebook(ngramrange):\n",
    "        text_stats[key][str(n_of_ngrams)+'-grams'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-13 07:50:06.649081 started calculating overall frequencies\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8504e15a6c0141f0b5c62f2b77256e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2627), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018-09-13 07:50:11.435217 ended calculating overall frequencies\n"
     ]
    }
   ],
   "source": [
    "n_of_text_files = 0\n",
    "step = len(file_ids) // 1000\n",
    "half = len(file_ids) // 2\n",
    "\n",
    "print(datetime.now(), 'started calculating overall frequencies')\n",
    "\n",
    "pool = ThreadPool(threads)\n",
    "results = pool.map(get_text_freq, tqdm_notebook(file_ids))\n",
    "pool.close()\n",
    "        \n",
    "print(datetime.now(), 'ended calculating overall frequencies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating normalized frequencies\n",
    "\n",
    "for token_length in text_stats['total']:\n",
    "    overall_freq = sum(text_stats['total'][token_length].values())\n",
    "    \n",
    "    for file_id in text_stats['total'][token_length]:\n",
    "        absfreq = text_stats['total'][token_length][file_id]\n",
    "        relfreq = Decimal(absfreq) / Decimal(overall_freq)\n",
    "        text_stats['total'][token_length][file_id] = {'absfreq': absfreq,\n",
    "                                                     'relfreq': float(relfreq)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_stats_insert = token_stats.insert_one({'_text-stats': True,\n",
    "                                  'text_stats': text_stats})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_stats = token_stats.find_one({'_text-stats': True})['text_stats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Calculate DP by Gries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Stephan Th. Gries's (2008, 2010) \"deviation of proportions\", in order to see how well dispersed tokens are in your corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dp(token):\n",
    "    \n",
    "    global text_stats, token_stats\n",
    "    \n",
    "    try:\n",
    "        # See if DP has already been calculated\n",
    "        dp = token_stats.find_one({'token': token})['dp']\n",
    "        return dp\n",
    "    \n",
    "    except TypeError:\n",
    "        # Token not found in DB\n",
    "        return float(1)\n",
    "    \n",
    "    except KeyError:\n",
    "        # DP was not calculated yet\n",
    "        # Let's calculate it then\n",
    "\n",
    "        try:\n",
    "            document = token_stats.find_one({'token': token})\n",
    "            token_length = document['len']\n",
    "            token_freq = document['freq']\n",
    "\n",
    "            freq_in_files = dict(zip(document['occurred_in'],document['freq_occurred_in']))\n",
    "\n",
    "            for file_id in freq_in_files:\n",
    "                freq_in_files[file_id] = Decimal(freq_in_files[file_id]) / Decimal(token_freq)\n",
    "\n",
    "            differences = float(0)\n",
    "\n",
    "            for file_id in text_stats['total'][str(token_length)+'-grams']:\n",
    "\n",
    "                expected_percentage = text_stats['total'][str(token_length)+'-grams'][file_id]['relfreq']\n",
    "                expected_percentage = abs(float(expected_percentage))\n",
    "                observed_percentage = abs(float(freq_in_files.get(file_id, float(0))))\n",
    "\n",
    "                diff = abs(expected_percentage - observed_percentage)\n",
    "                differences += diff\n",
    "            \n",
    "            dp = Decimal(differences) / Decimal(2)\n",
    "            \n",
    "            # Before returning DP, let us insert it to the DB\n",
    "            \n",
    "            result = token_stats.update_one({'token': token},\n",
    "                                    {'$set': {'dp': float(dp)}})\n",
    "\n",
    "            return float(dp)\n",
    "\n",
    "        except TypeError:\n",
    "            # Token not found in DB\n",
    "            return float(1)\n",
    "        \n",
    "        except KeyError:\n",
    "            return float(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this operation fails,\n",
    "# try dividing the MongoDB request more.\n",
    "# For instance:\n",
    "# first you run this cell for tokens with more than 1,000 occurrences\n",
    "# all_tokens = token_stats.distinct('token', {'freq': {'$gt': 1000}})\n",
    "# then you run this cell for tokens with less than 1,000 occurrences\n",
    "# all_tokens = token_stats.distinct('token', {'freq': {'$lt': 1000})\n",
    "# It will all depend on the size of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-09-13 08:58:53.014351 adding DP values for 4 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c2e8b147684e47ae4b8cb92cb9ca82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2018-09-13 08:58:53.085787 finished adding DP values for 4 tokens\n"
     ]
    }
   ],
   "source": [
    "all_tokens = token_stats.distinct('token', {'dp': {'$exists': False}})\n",
    "tokens_processed = 0\n",
    "\n",
    "print(datetime.now(), 'adding DP values for', len(all_tokens), 'tokens')\n",
    "pool = ThreadPool(threads)\n",
    "results = pool.map(calc_dp, tqdm_notebook(all_tokens))\n",
    "pool.close()\n",
    "print(datetime.now(), 'finished adding DP values for', len(all_tokens), 'tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
